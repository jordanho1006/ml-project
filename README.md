# ml-project
The following project is my attempt in dabble with Big Data. The dataset used is based on VTuber live chat statistics acquired from Kaggle. In this project, I try to utilize Spark on an Amazon EMR Cluster to process my data using machine learning and create data visualizations.

Dataset: https://www.kaggle.com/datasets/uetchy/vtuber-livechat
              https://www.kaggle.com/datasets/uetchy/vtuber-livechat

Milestone 1
  A VTuber or virtual Youtuber is a type of online entertainer who uses a virtual avatar opposed to regular YouTube content-creators who may have an open webcam to show their faces during streams. This digital trend started as far back as 2010 in Japan but as of early 2020s, VTubers have become an international phenomenon. That’s when I started to wonder how sustainable a career being a VTuber is if you’ve made it to the top. Afterall, a main source of their income ends up being super chats which are essentially donations which highlight the donator’s message. The dataset I’ve acquired from Kaggle called “VTuber 1B Elements: Live Chat Statistics” includes billions of live chat messages, “superchats” and moderation events like bans or deletions from VTuber livestreams. This dataset includes other attributes such as, but not limited to, timestamp, amount, currency type, significant and channel ID of the superchat. I will be including an additional CSV file so that I can join it together with the dataset. The CSV file takes the channel ID and gives us the channel name, affiliations, groups, and subscriber count. This will make it easier to distinguish who the superchat message is for instead of just looking at their Channel ID, making the data easier to read.
  I believe the best utilization of this dataset would be predicting the growth or maybe even the decline of super chats for VTubers in the coming future. It may be easy to determine how much VTubers earn by using the super chat data alone. However, it would be even more useful to use that data to predict future channel growth. This is especially true since VTubers growth at vastly different rates. There are various factors to why certain channels grow faster than others, perhaps the data will allow us to find out some of these factors like their time of debut or their region. Lastly, the data can provide information on which channels receive superchats from the same viewers. This data can turn out very useful because it can identify which collabs can promote most growth.

Milestone 2
Creating an Amazon S3 Bucket and download the dataset directly from the Kaggle API using an EC2 Instance. The code to do that is in hololive_superchat

Milestone 3
	Using Python code, I was able to acquire descriptive statistics on my Vtuber Live Chat Statistics dataset. However, to narrow down my search, I will only be focusing on Vtubers that are affiliated with Hololive Production. At the present time, there are fifty-four different Vtubers who are affiliated with Hololive. When Vtubers debut, they are typically in groups called generations. Currently, there are fourteen generations. This dataset contains 16 different data columns: timestamp, amount, currency, significance, authorChannelId, bodylength, impact, name, englishName, affiliation, group, SubscriptionCount, videoCount and photo. Additionally, descriptive statistics such as count or mean is provided in the images below.
	In the future, there will probably be many challenges when dealing with this data set. The main challenge would be finding a way around the memory limit for my EC2 instance. At the moment, I will have to limit myself to specific queries rather than general ones to prevent my instance from running out of memory and “killing” my Python command. In fact, I have already begun to run into this issue trying to unzip my files in my S3 folder. In addition, simply because of the sheer size of the data set, finding any irregularities in the data that may negatively influence my findings in the future will be a challenge as well. 

Milestone 4
Fortunely, I did not have many issues cleaning the data since that data was relatively straight forward. However, because the data was made up of many parquet files, I had to combine all the files into a single dataframe using the concat function. In addition, I had to bring in another CSV file that included channel names so that I could combine it with the dataset so that it was easy to identify which data belong to who rather than relying on unique channel ids. One thing I did notice that I did not notice when I initially looked at the data was that there were some NULL values in the “group” column which is unnatual since every member must belong to a group. Thus, after finding which member had a NULL value for group, I was able to deduce that only certain members from the “1st Generation” had this issue. So I was able to utilize pandas and replace any NULL values first “1st Generation” and saved it as a CSV file so I could get more accurate results in my feauture engineering process.

Milestone 5
Utilize feature engineering, machine learning and data visualizations to process the data. The code to do this can be seen in hololive_superchat.
